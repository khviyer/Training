# -*- coding: utf-8 -*-
"""chatbot_core.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ait44kjO1ad0r1O1VndRtS8GKeob1hkJ

## Import Required Libraries
"""

pip install texthero

pip install -U spacy

import io
import random
import string
import warnings
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import pandas as pd
import texthero as hero
from texthero import preprocessing
from scipy.spatial import distance
import re
from typing import List
import spacy
from spacy.tokens import Doc
from tqdm import tqdm

warnings.filterwarnings('ignore')
pd.set_option('display.max_colwidth', None)

import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('popular', quiet=True)

#nltk.download('punkt')
#nltk.download('wordnet')

"""## Data Extraction """

from google.colab import drive
drive.mount('/content/drive')

df_data = pd.read_csv('/content/drive/MyDrive/chatbot/data/chatbot_data_final.csv')
df_data['combined_content'] = df_data['CONTENT']
df_data = df_data[['combined_content']]

print(df_data.shape)
df_data.head()

"""## Preprocessing"""

df_null = df_data[df_data['combined_content'].isnull()]
print("Number of null records present: ", df_null.shape)

df_data = df_data[~df_data['combined_content'].isnull()]
print("Final dataset size after removing null records: ", df_data.shape)

def convert_lowercase(df):
  df['clean_content'] = preprocessing.lowercase(df['combined_content'].astype(str))
  return df

df_data = convert_lowercase(df_data)
df_data.head(2)

def remove_diacritics(df):
  df['clean_content'] = preprocessing.remove_diacritics(df['clean_content'].astype(str))
  return df

df_data = remove_diacritics(df_data)
df_data.tail(2)

def remove_brackets(df):
  df['clean_content'] = preprocessing.remove_brackets(df['clean_content'].astype(str))
  return df

df_data = remove_brackets(df_data)
df_data.tail(2)

def remove_whitespace(df):
  df['clean_content'] = preprocessing.remove_whitespace(df['clean_content'].astype(str))
  return df

df_data = remove_whitespace(df_data)
df_data.head(2)

def remove_urls(df):
  df['clean_content'] = preprocessing.remove_urls(df['clean_content'].astype(str))
  return df

df_data = remove_urls(df_data)
df_data.head(2)

from typing import Optional
def remove_punctuation_from_text(input_text: str, punctuations: Optional[str] = None) -> str:
    if punctuations is None:
        punctuations = string.punctuation
    processed_text = input_text.translate(str.maketrans('', '', punctuations))
    return processed_text

def remove_punctuation(df):
  custom_punctuation = '!"#$&\'()*+,.:;<=>?@[\\]^{|}~'
  df['clean_content'] = df.apply(lambda x: remove_punctuation_from_text(x['clean_content'], custom_punctuation), axis=1)
  return df

df_data = remove_punctuation(df_data) 
df_data.head(2)

def replace_irrelevant_string(df):
  df['clean_content'] = df['clean_content'].str.replace('k_', '', regex=False)
  df['clean_content'] = df['clean_content'].str.replace('nk_', '', regex=False)
  return df

df_data = replace_irrelevant_string(df_data)
df_data.head(2)

# Replace all _ with space
def replace_special_character(df):
  df['clean_content'] = preprocessing.replace_punctuation(df['clean_content'].astype(str))
  return df

df_data = replace_special_character(df_data)
df_data.head(2)

def word_tokenize(text_data):
  teken_list = nltk.word_tokenize(text_data)
  return teken_list

test_data = "escalator health score imact area performance reliability by field operationmode value"
tokens = word_tokenize(test_data)
tokens

def substitute_token(token_list: List[str], sub_dict: Optional[dict] = None) -> List[str]:
    replace_word_dict = {
        'asset':'equipment number',
        'resource_cd':'equipment number',
        'equipment_number':'equipment number',
        'ken':'equipment number',
        'ken_no':'equipment number',
        'sn':'service need',
        'rule code':'service need',
        'rule_code':'service need',
        'service_need':'service need',
        'sn_code':'service need',
        'service_need_code':'service need'
    }

    if token_list is None or len(token_list) == 0:
        return []
    processed_tokens = list()
    for token in token_list:
        if token in replace_word_dict:
            processed_tokens.append(replace_word_dict[token])
        else:
            processed_tokens.append(token)
    return processed_tokens

test_data = ['the', 'sn', 'tsn005086', 'is', 'related', 'to', 'hoisting', 'rope', 'in', 'planning', 'activity']
sub_tokens = substitute_token(test_data)
sub_tokens

"""### Common method for all the above preprocessing steps"""

def prerocess_data(df):
  df = convert_lowercase(df)
  df = remove_diacritics(df)
  df = remove_brackets(df)
  df = remove_whitespace(df)
  df = remove_urls(df)
  df = remove_punctuation(df)
  df = replace_irrelevant_string(df)
  df = replace_special_character(df)
  return df

df_data.head()

df_data_clean = prerocess_data(df_data)
df_data_clean.head()

def sentence_tokenize(df):
  sent_token = df['clean_content'].tolist()
  return sent_token

sent_tokens = sentence_tokenize(df_data_clean)
sent_tokens[:2]

lemmer = WordNetLemmatizer()

def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]

def LemNormalize(text):
    text = word_tokenize(text)
    text = substitute_token(text)
    return LemTokens(text)

"""## Chatbot Response """

def prepare_user_response(text_data):
  data = np.array([text_data])
  df_response = pd.DataFrame(data, columns=['combined_content'])
  df_response = prerocess_data(df_response)
  response_cleaned = df_response['clean_content'].to_string(index=False)
  return response_cleaned

prepare_user_response('What is the significance of health score in planning solution ?')

# Generating response
def response(user_response):
    chatbot_response=''
    sent_tokens.append(user_response)
    
    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')
    tfidf = TfidfVec.fit_transform(sent_tokens)
    
    vals = cosine_similarity(tfidf[-1], tfidf)
    idx = vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    
    if(req_tfidf == 0):
        chatbot_response = chatbot_response + "I am sorry! I don't understand you"
    else:
        chatbot_response = chatbot_response + sent_tokens[idx]
    
    return chatbot_response

greeting_inputs = ("hello", "hi", "greetings", "what's up","hey")
greeting_responses = ["hi", "hey", "hi there", "hello", "I am glad! You are talking to me"]

feedback_inputs_positive = ('yes', 'good', 'satisfied', 'not bad')
feedback_inputs_negative = ('no', 'bad', 'not good', 'displeasure', 'discontent')

def greeting(sentence):
    for word in sentence.split():
        if word.lower() in greeting_inputs:
            return random.choice(greeting_responses)

def planning_solution_assistant():
    flag=True
    print("Planning Assistant: I am Planning Soution Assistant. I will answer your queries about Planning Soution. If you want to exit, type Bye!")

    df_chat = pd.read_csv('/content/drive/MyDrive/chatbot/chat_history/chat_history.csv')
    chat = {}

    while(flag == True):
        user_response = input()
        user_response = prepare_user_response(user_response)

        if(user_response != 'bye'):
            if(user_response == 'thanks' or user_response == 'thank you' ):
                flag=False
                print("Planning Assistant: You are welcome..")

            elif(greeting(user_response) != None):
                print("Planning Assistant: " + greeting(user_response))

            else:
                if(user_response in feedback_inputs_positive):
                    flag=False
                    print("Planning Assistant: Thank you for using Planner Assistant.")
                    chat['FEEDBACK'] = 1

                elif(user_response in feedback_inputs_negative):
                    flag=False
                    print("Planning Assistant: Sorry for the dissatisfactory response. Thank you for using Planner Assistant.")
                    chat['FEEDBACK'] = 0

                else:
                    answer = response(user_response)
                    print("Planning Assistant: ",end = "")
                    print(answer + ".\nAre you satisfied with this answer ?")
                    sent_tokens.remove(user_response)
                    chat['QUESTION'] = user_response
                    chat['ANSWER'] = answer
            
        else:
            flag=False
            print("Planning Assistant: Bye! take care..")

    df_chat = df_chat.append(chat, ignore_index=True)     
    df_chat.to_csv('/content/drive/MyDrive/chatbot/chat_history/chat_history.csv', index=False)

"""## Run Chatbot """

planning_solution_assistant()

